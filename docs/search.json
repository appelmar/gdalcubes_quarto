[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The gdalcubes project is maintained and developed by Marius Appel."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nInstitute for Geoinformatics\nUniversity of Muenster\nHeisenbergstraße 2\nD-48149 Münster\nGermany\n\nMail: marius.appel@uni-muenster.de\nPhone: +49 251 83 33082\n\nORCID ID: 0000-0001-5281-3896"
  },
  {
    "objectID": "source/introduction/overview.html",
    "href": "source/introduction/overview.html",
    "title": "gdalcubes",
    "section": "",
    "text": "The core part of gdalcubes is implemented as a C++ library, linking to GDAL, NetCDF, SQLite, and libcurl.\nAmong others, the library provides data structures for image collections, raster data cubes, and implements the creation of data cubes and some functions on data cubes (see Basic concepts).\nAn executable command line interface links to the gdalcubes library and provides functionality to create and modify image collections, and to execute chains of data cube operations. It is mainly used for debugging and profiling.\ngdalcubes includes an experimental server application, exposing parts of the library functions as a REST-like web service. The web service is mainly used for distributed processing where server instances participate as worker nodes.\nThe R package targets at data scientists and provides an easy to use interface to the gdalcubes library. It is, however, mostly a wrapper, i.e. does not add much functionality. The package includes the sources of the core library and does not link to the separately built library.\nThe figure below summarizes components of gdalcubes, how they are related, and their external dependencies.\n\n\n\n\n\n\nNote\n\n\n\nSome external libraries are included in the sources, i.e., they will be compiled together with gdalcubes, and do not need to be built separately.\n\n\n\n\n\nFigure 1: Components and dependencies of gdalcubes."
  },
  {
    "objectID": "source/introduction/license.html",
    "href": "source/introduction/license.html",
    "title": "gdalcubes",
    "section": "",
    "text": "MIT License\nCopyright (c) 2019-2022 Marius Appel marius.appel@uni-muenster.de\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "source/introduction/credits.html",
    "href": "source/introduction/credits.html",
    "title": "gdalcubes",
    "section": "",
    "text": "gdalcubes wouldn’t exist without other open-source projects. This document presents a list of used third-party software including their purpose, copyright, and licensing information in no particular order. Please notice that some libaries are only used by the command line client or gdalcubes_server, but not by the core library.\n\nGDAL: A translator library for raster and vector geospatial data formats\n\nCopyright (c) 2000, Frank Warmerdam\nCopyright (c) The GDAL/OGR project team\nLicense: MIT (https://opensource.org/licenses/MIT)\nParts of GDAL are licensed under different terms, see https://github.com/OSGeo/gdal/blob/master/gdal/LICENSE.TXT\ngdalcubes may statically or dynamically link to the gdal library depending on compilation flags\n\njson11\n\nCopyright (c) 2013 Dropbox, Inc.\nLicense: MIT (https://opensource.org/licenses/MIT)\ngdalcubes distributes an unmodified version of the library under src/external/json11/*\n\nSQLite: A self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine\n\nCopyright: public domain\nLicense: public domain\ngdalcubes may statically or dynamically link to the sqlite library depending on compilation flags\n\nCURL: Command line tool and library for transferring data with URLs\n\nCopyright (c) 1996 - 2018, Daniel Stenberg\nLicense: curl license(https://curl.haxx.se/docs/copyright.html)\ngdalcubes may statically or dynamically link to the libcurl library depending on compilation flags\n\nTinyExpr: A very small recursive descent parser and evaluation engine for math expressions\n\nCopyright (c) 2015-2018 Lewis Van Winkle\nLicense: zlib (https://opensource.org/licenses/Zlib)\ngdalcubes distributes a modified version of the library under src/external/tinyexpr, modifications include the implementation of logical and bit-wise operators and remove ISO C -Wpedantic warnings.\n\nnetCDF: The Unidata network Common Data Form C library\n\nCopyright (c) 1993-2017 University Corporation for Atmospheric Research/Unidata\nLicense: MIT-like, see https://www.unidata.ucar.edu/software/netcdf/copyright.html\nDOI: http://doi.org/10.5065/D6H70CW6\ngdalcubes may statically or dynamically link to the NetCDF C library depending on compilation flags\n\ntiny-process-library: A small platform independent library making it simple to create and stop new processes in C++\n\nCopyright (c) 2015-2018 Ole Christian Eidheim\nLicense: MIT (https://opensource.org/licenses/MIT)\n\ngdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags\ngdalcubes distributes a modified version of the library under src/external/tiny-process-library, modifications are limited to replacing _exit() and exit() calls by raise(SIGKILL), in order to comply with R CMD check.\n\nCatch2: A modern, C++-native, header-only, test framework for unit-tests, TDD and BDD\n\nCopyright (c) 2010 Two Blue Cubes Ltd\nLicense: Boost Software License 1.0 (https://www.boost.org/LICENSE_1_0.txt)\ngdalcubes distributes an unmodified version of the library under src/external/catch.hpp\n\nBoost.Filesystem\n\nCopyright (c) Beman Dawes, 2011\nLicense: Boost Software License 1.0 (https://www.boost.org/LICENSE_1_0.txt)\ngdalcubes may statically or dynamically link to the Boost.Filesystem library depending on compilation flags\n\nBoost.Program_options\n\nCopyright (c) 2002-2004 Vladimir Prus\nLicense: Boost Software License 1.0(https://www.boost.org/LICENSE_1_0.txt)\n\ngdalcubes may statically or dynamically link to the Boost.Program_options library depending on compilation flags\n\nDate: A date and time library based on the C++11/14/17 <chrono> header\n\nCopyright (c) 2015, 2016, 2017 Howard Hinnant\nCopyright (c) 2016 Adrian Colomitchi\nCopyright (c) 2017 Florian Dang\nCopyright (c) 2017 Paul Thompson\nCopyright (c) 2018 Tomasz Kamiński\n\nLicense: MIT (https://opensource.org/licenses/MIT)\n\ngdalcubes distributes a modified version of the library under src/external/date.h, modifications are limited to commenting out diagnostic pragmas, and adding a compilation flag for enabling __int128 support (see https://github.com/appelmar/gdalcubes/commit/9e936155489a97c5bb211e13e757236762ee1d96)\n\ncpprestsdk\n\nCopyright (c) Microsoft Corporation\nLicense: MIT (https://opensource.org/licenses/MIT)\n\ngdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags\n\n\nDerived work such as R or Python packages may use further external software (see their documentation)."
  },
  {
    "objectID": "source/introduction/installation.html",
    "href": "source/introduction/installation.html",
    "title": "gdalcubes",
    "section": "",
    "text": "gdalcubes can be compiled from sources via CMake. CMake automatically checks for mandatory and optional dependencies and adapts the build configuration. The following commands install gdalcubes from sources.\n  git clone https://github.com/appelmar/gdalcubes && cd gdalcubes\n  mkdir -p build \n  cd build \n  cmake -DCMAKE_BUILD_TYPE=Release ../ \n  make \n  sudo make install\nIf any of the required libraries are not available on your system, please use your package manager to install these before, e.g. with (Ubuntu):\nsudo apt-get install libgdal-dev libnetcdf-dev libcurl4-openssl-dev libsqlite3-dev\n\n# additional installs for command line interface \nsudo apt-get install libboost-program-options-dev libboost-system-dev\n\n# additional installs for server executable\nsudo apt-get install libcpprest-dev\nPlease notice we have not yet tried to build with Microsoft Visual Studio. However, the R package installation from sources includes building the C++ library and works on Windows, MacOS and Linux (see below).\n\n\nThe Dockerfile at the root of the project is built on a minimal Ubuntu installation, including installation of all dependencies and compiles gdalcubes from sources automatically.\ngit clone https://github.com/appelmar/gdalcubes && cd gdalcubes \ndocker build -t appelmar/gdalcubes .\ndocker run -d -p 11111:1111 appelmar/gdalcubes # runs gdalcubes_server as a deamon \ndocker run appelmar/gdalcubes /bin/bash # run a command line where you can run gdalcubes \n\n\n\n\nThe R package can be installed from CRAN including binary package builds for MacOS and Windows:\ninstall.packages(\"gdalcubes\")\nAlternatively, to install the development version from sources is easiest with\nremotes::install_git(\"https://github.com/appelmar/gdalcubes_R\")\nPlease make sure that the git command line client is available on your system. Otherwise, the above command might not clone the gdalcubes C++ library as a submodule under src/gdalcubes. The package links to the external libraries GDAL, NetCDF, and SQLite (see below).\n\n\nOn Windows, you will need Rtools to compile the package from sources. System libraries are automatically downloaded from rwinlib.\n\n\n\nPlease install the system libraries e.g. from the package manager of your Linux distribution. Also make sure that you are using a recent version of GDAL (> 2.3.0). On Ubuntu, the following commands install all required libraries.\nsudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update\nsudo apt-get install libgdal-dev libnetcdf-dev libsqlite3-dev libudunits2-dev\n\n\n\nUse Homebrew to install system libraries with\nbrew install pkg-config\nbrew install gdal\nbrew install netcdf\nbrew install libgit2\nbrew install udunits\nbrew install curl\nbrew install sqlite"
  },
  {
    "objectID": "source/introduction/faq.html",
    "href": "source/introduction/faq.html",
    "title": "gdalcubes",
    "section": "",
    "text": "Most likely, not out of the box! gdalcubes comes with a few predefined image collection formats for Sentinel-2, Landsat, MODIS, and a few other datasets only. You will need to write a custom image collection format (see here). Please share your collection formats via a pull request on the GitHub collection format respository.\n\n\n\nContributions of any kind (whether fixing typos, improving documentation, reporting bugs, asking questions, or implementing new features) are very welcome. Please use issues and pull requests on GitHub.\n\n\n\nPlease refer to our paper at https://doi.org/10.3390/data4030092. You can use the corresponding BibTeX entry:\n    @Article{data4030092,\n        AUTHOR = {Appel, Marius and Pebesma, Edzer},\n        TITLE = {On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library},\n        JOURNAL = {Data},\n        VOLUME = {4},\n        YEAR = {2019},\n        NUMBER = {3},\n        ARTICLE-NUMBER = {92},\n        URL = {https://www.mdpi.com/2306-5729/4/3/92},\n        ISSN = {2306-5729},\n        DOI = {10.3390/data4030092}\n    }"
  },
  {
    "objectID": "source/introduction/why.html",
    "href": "source/introduction/why.html",
    "title": "gdalcubes",
    "section": "",
    "text": "Collections of satellite imagery are difficult to analyze because of their complex structure:\n\nspectral bands of one image may have different pixel sizes,\ntwo or more images may spatially overlap, have different spatial reference systems, and yield irregular time series,\nImages from different data products (or different satellites) do not align in space and time, and use different data formats.\n\nThe Geospatial Data Abstraction Library (GDAL) can be used to solve some of these problems by abstracting from particular file formats and providing high performance image warping (reprojection, rescaling, cropping and reampling), but does not know about image time series.\ngdalcubes builds on top of GDAL and aims at making the work with large satellite image collections easier, faster, more intuitive, and more interactive."
  },
  {
    "objectID": "source/concepts/datacubes.html",
    "href": "source/concepts/datacubes.html",
    "title": "gdalcubes",
    "section": "",
    "text": "References\n\nAppel, Marius, and Edzer Pebesma. 2019. “On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library.” Data 4 (3). https://doi.org/10.3390/data4030092."
  },
  {
    "objectID": "source/concepts/distributed.html",
    "href": "source/concepts/distributed.html",
    "title": "gdalcubes",
    "section": "",
    "text": "Warning\n\n\n\nFeatures described here are experimental and have not been implemented in the R package.\n\n\nData cubes can be processed in distributed computing environments, where nodes communicate over HTTP.\nOne coordinator needs to define a swarm of workers, each running the gdalcubes server application. The coordinator then simply distributes chunks to be processed to the available workers. This basically involves three steps:\n\nSend the JSON process graph and required additional files (e.g. image collections) to all worker nodes.\nDistribute chunks to individual workers, which then start processing.\nWaits for the workers, download the data and e.g. write to the output netCDF file.\n\n\n\n\n\n\n\nNote\n\n\n\nSince image collections are shared, links to GDAL datasets must be accessible on all worker nodes.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe current implementation is not robust against failures or unavailability of workers.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDistributed processing is currently not implemented for streaming operations.\n\n\n\n\nThe worker instances are running the server executable, exposing a a REST-like API for communication. The following sections describes available endpoints. By default, the gdalcubes_server executable will listen on http://0.0.0.0:1111/gdalcubes/api/.\n\n\nReturns version information as text.\n\n\n\nChecks whether the server instance has a file with name and size equal to the given query parameters. Returns 200 (OK), if the file is available, 409 (Conflict) if a file with the same name has different size, 400 (Bad Request) if size and / or name is missing in the request, or 204 (No Content) if the file does not exist.\n\n\n\nUploads the file in the application/octet-stream body as file with name as given in the query parameters.\n\n\n\nUploads a JSON description of a cube and returns a cube ID.\n\n\n\nQueue a chunk read for a given cube and given chunk id as given in the path, returns immediately.\n\n\n\nAsk for the status of a chunk read request. Possible return values are notrequested, queued, running, finished, and error.\n\n\n\nDownload a chunk with given id as application/octet-stream. The chunk must have been queued before. If the chunk has not yet been read, it will block until the data becomes available."
  },
  {
    "objectID": "source/concepts/operations.html",
    "href": "source/concepts/operations.html",
    "title": "gdalcubes",
    "section": "",
    "text": "gdalcubes includes some built-in functions to process data cubes.\nThe table below lists available operations that take one (or more) data cube(s) as input and produce a single result data cube.\n\n\n\n\nTable 1: Built-in data cube operations\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nselect_bands\nSelect a subset of a data cube’s bands.\n\n\nreduce_time\nApply a reducer function over individual pixel time series.\n\n\nreduce_space\nApply a reducer function over spatial slices of a data cube.\n\n\napply_pixel\nApply a arithmetic expression on band values per data cube pixel.\n\n\nfilter_pixel\nFilter pixels by a logical predicate on band values.\n\n\nwindow_time\nApply a moving window aggregate or kernel on individual pixel time series.\n\n\nfill_time\nFill missing values of a data cube by simple time series interpolation.\n\n\njoin_bands\nCombine bands of two identically shaped data cubes.\n\n\nfilter_geom\nTODO\n\n\ncrop\nTODO\n\n\nselect_time\nTODO\n\n\nslice_time\nTODO\n\n\nslice_space\nTODO\n\n\naggregate_time\nTODO\n\n\n\n\nInternally these operations inherit from the general cube class. They provide a cheap create method, returning a proxy object without running any expensive computations on the data.\nThis makes it possible to define and optimize chains of operations, where eexecution is delayed until users call a method that reads data of the resulting data cube (such as plotting, or exporting a derived data cube as a file, see Data Cube Export_). Below, you can find examples in C++ and R, how data cube operations can be chained programmatically.\n\n\n\n\nC++R\n\n\n#include <gdalcubes.h>\n   \nint main() {\n\n   // load an existing image collection\n   auto ic = std::make_shared<gdalcubes::image_collection>(\"test.db\");\n   \n   // define a data cube view\n   gdalcubes::cube_view v;\n   v.srs() = \"EPSG:4326\";\n   v.left() = -180;\n   v.top() = 60;\n   v.bottom() = -60;\n   v.right() = 180;\n   v.t0() = gdalcubes::datetime::from_string(\"2018-01-01\");\n   v.t1() = gdalcubes::datetime::from_string(\"2018-12-31\");\n   v.dx(0.1);\n   v.dy(0.1);\n   v.dt(gdalcubes::duration::from_string(\"P1D\"));\n \n   // chain data cube operations\n   auto x = gdalcubes::image_collection_cube(ic, v);\n   auto x_bands = gdalcubes::select_bands_cube::create(x, {\"B04\", \"B08\"});\n   auto x_ndvi = gdalcubes::apply_pixel_cube::create(x_bands, {\"(B08-B04)/(B08+B04)\"}, {\"NDVI\"});\n   auto x_reduced =  gdalcubes::reduce_time_cube::create(x_ndvi, {{\"max\", \"NDVI\"}});\n\n   // export result cube as a netCDF file\n   x_reduced->write_netcdf_file(\"test.nc\");\n\n   return 0;\n}\n\n\nlibrary(gdalcubes)\n\nic = image_collection(\"test.db\")\nv = cube_view(srs = \"EPSG:4326\", dx = 0.1, dy = 0.1, dt = \"P1D\", \n             extent = list(left = -180, right = 180, top = 60, bottom = -60,\n                           t0 = \"2018-01-01\", t1 = \"2018-12-31\"))\n                        \nraster_cube(ic, v) |>\n select_bands(c(\"B04\", \"B08\")) |>\n apply_pixel(\"(B08-B04)/(B08+B04)\",\"NDVI\") |>\n reduce_time(\"median(NDVI)\") |>\n write_ncdf(\"test.nc\")\n\n\n\n\n\n\nData cubes can be exported as single netCDF files, or as a collection of (possibly cloud-optimized) GeoTIFF files.\n\n\nProduced netCDF files use the enhanced netCDF-4 data model, follow the CF-1.6 conventions <http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html>_ and include bands of the data cube as variables. Dimensions are in the order time, y, x or time, lat, lon respectively.\nUsers can pass additional options to control properties of the resulting netCDF file:\n\nDEFLATE compression can be specified by integer values between 0 (no compression) and 9 (maximum compression).\nUsers can furthermore enable or disable the inclusion of boundary variables in the output netCDF file. These variables give exact upper and lower limits of dimension values (e.g. start and end time of a pixel).\n\n\n\n\nExporting a data cube as a collection of GeoTIFF files will produce one multiband GeoTIFF file per time slice of the data cube. Files will be named by a user defined prefix and the date/time of the slice, such as “cube_2018-01.tif”.\nUsers can pass additional options to control properties of the resulting GeoTIFF files:\n\nSetting overviews = true will generate GDAL overviews (image pyramids) for the resulting files. Overview levels reduce the pixel size by powers of 2, until width and height are lower than or equal to 256. The resampling algorithm for generating overviews can be selected from what GDAL offers (see here <https://gdal.org/programs/gdaladdo.html#cmdoption-gdaladdo-r>__, default is nearest neighbor).\nSetting COG = true will produce cloud-optimized <https://www.cogeo.org/>_ GeoTIFF files and includes overviews = true.\nAdditional GDAL creation options can be passed, e.g., to define LZW or DEFLATE compression (see here <https://gdal.org/drivers/raster/gtiff.html#creation-options>__).\n\n\n\n\nBoth, GeoTIFF as well as netCDF export support packing data, i.e. reducing the size of the output file(s) by conversion of double values to smaller integer types using an affine transformation of original values by an offset and scale, where values are transformed according to the formula:\n.. math:: x_{unpacked} = x_{packed} scale + offset\nPacking values reduces the precision of the data. Packing in gdalcubes requires users to pass\n\nthe output data type (one of uint8 , uint16, int16, uint32, int32, float32).\nscale, offset, and nodata values (unless type is float32 ).\n\nNaN values of data cubes will automatically converted to provided nodata values in the target data type. Individual scale, offset, and nodata values can be specified either once (applied for all bands), or individually per band. Packing to type float32 will ignore any of the offset, scale, and nodata values but implicitly cast 8 byte doubles to 4 byte float values.\n\n\n\n\nData cube operations can be chained, producing a directed acyclic graph. This graph can be serialized as JSON document, that can be useful to recreate data cubes, or keeping track of how a dataset has been generated. The following JSON document is an example, defining a median reduction of NDVI pixels on a yearly data cube over a small region in the Brazilian Amazon forest.\n.. code-block:: json :caption: Example process graph serialized as JSON document.\n{\n\"cube_type\": \"reduce_time\",\n\"in_cube\": {\n    \"band_names\": [\n    \"NDVI\"\n    ],\n    \"cube_type\": \"apply_pixel\",\n    \"expr\": [\n    \"(b05-b04)/(b05+b04)\"\n    ],\n    \"in_cube\": {\n    \"bands\": [\n        \"B04\",\n        \"B05\"\n    ],\n    \"cube_type\": \"select_bands\",\n    \"in_cube\": {\n        \"chunk_size\": [\n        16,\n        256,\n        256\n        ],\n        \"cube_type\": \"image_collection\",\n        \"file\": \"/tmp/RtmpacmRAy/file11af57b771e8.sqlite\",\n        \"view\": {\n        \"aggregation\": \"median\",\n        \"resampling\": \"bilinear\",\n        \"space\": {\n            \"bottom\": -550000.0,\n            \"left\": -6180000.0,\n            \"nx\": 2000,\n            \"ny\": 2000,\n            \"right\": -6080000.0,\n            \"srs\": \"EPSG:3857\",\n            \"top\": -450000.0\n        },\n        \"time\": {\n            \"dt\": \"P1Y\",\n            \"t0\": \"2014\",\n            \"t1\": \"2018\"\n        }\n        },\n        \"warp_args\": []\n    }\n    },\n    \"keep_bands\": false\n},\n\"reducer_bands\": [\n    [\n    \"median\",\n    \"NDVI\"\n    ]\n]\n}"
  },
  {
    "objectID": "source/concepts/collection_formats.html",
    "href": "source/concepts/collection_formats.html",
    "title": "gdalcubes",
    "section": "",
    "text": "Image collection formats describe how image collections are composed from GDAL datasets, considering specific data product organizations.\nA collection format is a JSON file defining rules, how to derive required metadata when a GDAL dataset (specified by its identifier, e.g. the path to a local GeoTIFF file) is added to an image collection.\nWhen adding a GDAL dataset (given its identifier) to a collection, we must find out\n\nto which image the dataset belongs (identified by a unique image name),\nthe recording date/time of the dataset, and\nhow bands of the dataset relate to bands of the collection.\n\n\n\nRules are mostly defined as regular expresions on the GDAL dataset identifiers. Some expressions simply check whether a dataset matches the expression or not, whereas some expression return a captured substring (e.g. the datetime part). gdalcubes uses the standard C++11 regular expression library with default ECMAScript syntax.\n\n\n\n\n\nThe JSON files for image collection formats start with some header information, including a short description of the particular product it adresses and some keywords. These header information are optional, but may be useful for searching.\nFor example, the following description and tags are used in the collection format for Sentinel 2 Level 2A products.\n{\n    \"description\" : \"Image collection format for Sentinel 2 Level 2A data as downloaded from the Copernicus Open Access Hub, expects a list of file paths as input. The format should work on original ZIP compressed as well as uncompressed imagery.\",\n    \"tags\" : [\"Sentinel\", \"Copernicus\", \"ESA\", \"BOA\", \"Surface Reflectance\"]\n}\n\n\n\nCollection formats include a global pattern as a regular expression such that input GDAL datasets not matching the regular expression will simply be ignored (quitly, without throwing exceptions). As a simple example, setting \"pattern\" : \".*\\\\.tif$\" would ignore files that do not end with “.tif”.\n\n\n\nImages can be composed from one or more GDAL datasets, where all datasets share the spatial extent, the spatial reference system, and acquisition date/time, but different datasets contain data for different bands.\ngdalcubes uses a regular expression to extract an image name from the GDAL dataset identifier. The extracted name must be identical for all datasets belonging to the same image.\nTo extract the image name from the identifier, the first marked subexpression (or capturing group) of the provided regular expression under \"images\" : {\"pattern\": \"REGEX\"} is used, i.e., a part in the expression within the first pair of parentheses.\n\n\n{\"images\":{\"pattern\":\".*(L[OTC]08_.{4}_.{6}_.{8}_.{8}_.{2}_.{2})[A-Za-z0-9_]+\\\\.tif\"}}\n\n\n\n\n\n\nExample input GDAL dataset identifiers and extracted image name (bold)\n\n\n\n\n\n“/path/to/ LC08_L1TP_226063_20140719_20170421_01_T1 _sr_band2.tif”\n“/path/to/ LC08_L1TP_226063_20140820_20170420_01_T1 _sr_aerosol.tif”\n“/path/to/ LC08_L1TP_227064_20130621_20170503_01_T1 _pixel_qa.tif”\n\n\n\n\n\n\n\n{\"images\":{\"pattern\":\"HDF4_EOS:EOS_GRID:\\\".+(MOD13A2\\\\.A.+)\\\\.hdf.*\"}}\n\n\n\n\n\n\nExample input GDAL dataset identifiers and extracted image name (bold)\n\n\n\n\n\n“HDF4_EOS:EOS_GRID:\\”/path/to/ MOD13A2.A2013353.h00v08.006.2018226105756 .hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days NDVI”\n“HDF4_EOS:EOS_GRID:\\”/path/to/ MOD13A2.A2013353.h20v03.006.2018226105924 .hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days NDVI”\n“HDF4_EOS:EOS_GRID:\\”/path/to/ MOD13A2.A2015033.h23v10.006.2015296122819 .hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days VI Quality”\n\n\n\n\n\n\n\n\nIn the current version of gdalcubes, the acquisition date/time of images is derived from the dataset identifiers. Similar to the extraction of image names, a pattern defines a regular expression where the first marked subexpression / capturing group within parentheses is extracted. The format field in datetime JSON object then defines how to convert the extracted string to a date/time object, according to the strptime function.\n\n\n{\n   \"datetime\" : {\n       \"pattern\" : \".*MSIL2A_(.+?)_.*\\\\.zip.*\",\n       \"format\" : \"%Y%m%dT%H%M%S\"}  \n} \n\n\n\n\n\n\n\nExample input GDAL dataset identifiers and extracted date/time string (bold)\n\n\n\n\n\n“/vsizip//path/to/S2A_MSIL2A_ 20180328T093031 _N0207_R136_T34UFD_20180328T145945.zip/S2A_MSIL2A_20180328T093031_N0207_R136_T34UFD_20180328T145945.SAFE/GRANULE/L2A_T34UFD_A014433_20180328T093032/IMG_DATA/R20m/T34UFD_20180328T093031_B8A_20m.jp2”\n“/vsizip//path/to/S2A_MSIL2A_ 20180328T093031 _N0207_R136_T34UFD_20180328T145945.zip/S2A_MSIL2A_20180328T093031_N0207_R136_T34UFD_20180328T145945.SAFE/GRANULE/L2A_T34UFD_A014433_20180328T093032/IMG_DATA/R20m/T34UFD_20180328T093031_SCL_20m.jp2”\n“/vsizip//path/to/S2A_MSIL2A_ 20180430T094031 _N0207_R036_T34UGD_20180430T114456.zip/S2A_MSIL2A_20180430T094031_N0207_R036_T34UGD_20180430T114456.SAFE/GRANULE/L2A_T34UGD_A014905_20180430T094109/IMG_DATA/R10m/T34UGD_20180430T094031_B08_10m.jp2”\n\n\n\n\n\n\n\n\n{\n  \"datetime\" : {\n      \"pattern\" : \".*M[OY]D13A2\\\\.A(.{7}).*\",\n      \"format\" : \"%Y%j\"} \n}\n\n\n\n\n\n\nExample input GDAL dataset identifiers and extracted date/time string (bold)\n\n\n\n\n\n“HDF4_EOS:EOS_GRID:\\”/path/to/MOD13A2.A 2013353 .h00v08.006.2018226105756.hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days NDVI”\n“HDF4_EOS:EOS_GRID:\\”/path/to/MOD13A2.A 2013353 .h20v03.006.2018226105924.hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days NDVI”\n“HDF4_EOS:EOS_GRID:\\”/path/to/MOD13A2.A 2015033 .h23v10.006.2015296122819.hdf\\“:MODIS_Grid_16DAY_1km_VI:1 km 16 days VI Quality”\n\n\n\n\nSome MODIS and climate model output datsets are provided as multidimensional HDF or netCDF files, where one file (or GDAL dataset) contains space and time of one variable. In these cases, GDAL exposes the additional time dimension as bands. For such datasets, gdalcubes supports the additional definition of the duration of a band. This duration is then added to the offset date/time, which is extracted from the filename as described above.\n\n\n\n{\n  \"datetime\" : {\n      \"pattern\" : \".*pr_day_HadGEM2-ES_historical_r1i1p1_EWEMBI_([0-9]{8}).*\",\n      \"format\" : \"%Y%m%d\",\n      \"bands\" : {\n          \"dt\" : \"P1D\" }}\n}\nIn this example the 10th band (zero-based) for example represents 10 days after the date extracted from the filename.\n\n\n\n\n\n\nNote\n\n\n\nThis feature is experimental and only available in the development version.\n\n\n\n\n\n\nThe bands object lists the bands of a data product as key value pairs, where the key is a unique band name, and the value is a JSON object with a pattern and optional other fields. The pattern field again defines a regular expression. If a GDAL dataset identifier matches the pattern, it is considered to contain data of the band. An dataset identifier may match the pattern of several bands (sometimes, all bands even define the same pattern) if a single input GDAL dataset contains multiple bands. In this case, the additional field band can be used to describe which internal band corresponds to the band of the image collection. band can be a one-based integer number and is identical to the band number of GDAL (as from running gdalinfo).\nSome additional per-band metadata fields may be added to band definitions. In the current version, these include nodata, scale, offset, and unit. If these values are not provided, they are derived from the GDAL metadata (which may or may not be defined).\n\n\nThe following example can be used to define some of the bands of Landsat 8 surface reflectance data, where each band is stored in a separate GeoTIFF file.\n{\n    \"bands\": {\n        \"B01\" : {\n            \"pattern\" : \".+sr_band1\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B02\" : {\n            \"pattern\" : \".+sr_band2\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B03\" : {\n            \"pattern\" : \".+sr_band3\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B04\" : {\n            \"pattern\" : \".+sr_band4\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B05\" : {\n            \"pattern\" : \".+sr_band5\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B06\" : {\n            \"pattern\" : \".+sr_band6\\\\.tif\",\n            \"nodata\" : -9999},\n        \"B07\" : {\n            \"pattern\" : \".+sr_band7\\\\.tif\",\n            \"nodata\" : -9999},\n        \"...\"\n    }\n}\n\n\n\nThe following example can be used to define bands of PlanetScope surface reflectance data, where all bands (except a mask band) are stored in a single GeoTIFF file.\n {\n      \"bands\" : {\n          \"red\" : {\n              \"nodata\" : 0,\n              \"pattern\" : \".+_AnalyticMS_SR\\\\.tif$\",\n              \"band\" : 3},\n          \"green\" : {\n              \"nodata\" : 0,\n              \"pattern\" : \".+_AnalyticMS_SR\\\\.tif$\",\n              \"band\" : 2},\n          \"blue\" : {\n              \"nodata\" : 0,\n              \"pattern\" : \".+_AnalyticMS_SR\\\\.tif$\",\n              \"band\" : 1},\n          \"nir\" : {\n              \"nodata\" : 0,\n              \"pattern\" : \".+_AnalyticMS_SR\\\\.tif$\",\n              \"band\" : 4},\n          \"...\"\n      }\n }\n\n\n\n\nFor some data products (such as MODIS), bands are distributed as GDAL subdatasets within a single file (typically NetCDF or HDF). Adding \"subdatasets\" : true to the top-level JSON object makes sure that all subdatasets will be considered automatically. Other patterns of the collection format to extract image names amd date/time then relate to subdataset identifiers.\n\n\n\nSome global climate data products distributed as NetCDF files do not encode the SRS of images in a way that GDAL reads automatically. In these cases the collection format allows to define a global spatial reference system that overwrites the reference system of individual images (if available). For example, adding \"srs\" : \"EPSG:4326\" to the top-level JSON object makes sure that the WGS84 spatial reference system will be used for all images.\n\n\n\nIt is possible to extract further per-image metadata key value pairs from GDAL datasets. The collection format may include an optional field “image_md_fields” to list metadata keys as a JSON array of strings. When GDAL datasets are opened, GDAL tries to find the corresponding metadata keys and stores corresponding values in the image metadata table of the image collection.\nMetadata fields may be separated by domains (see GDAL metadata model). If metadata fields from a specific domain are needed, you can use DOMAIN:KEY, such as IMAGERY:CLOUDCOVER. The example below could be used to get some per-image quality flags from MODIS metadata.\n{\"image_md_fields\" : [\"PERCENTLAND\", \"QAPERCENTGOODQUALITY\", \"QAPERCENTNOTPRODUCEDCLOUD\"]}\n\n\n\n\nComplete examples of image collection formats can be found in the sources. There is also a dedicated GitHub repository\n\n\n\nWhen writing a collection format for new data products, please make sure to check the following notes:\n\nIf available, read the data product handbook. Most official satellite image product handbooks include a section on filenaming conventions.\nFor portability of local file-based image collections, make sure that preceding directory names (e.g. “C:\\Users\\”, or “/home/user/data”) do not matter to successfully create image collections.\n\nIf possible, avoid using path separators in regular expressions or use non-capturing alternation (?:/|\\\\) if you have to.\nImage collections do not need to include data for all bands. It is recommended to list all possible bands of a data product in the format. For example, Landsat 8 surface reflectance products may or may not include additional precomputed spectral index bands. To be able to use these bands if available, they must be listed in the collection format."
  },
  {
    "objectID": "source/concepts/image_collections.html",
    "href": "source/concepts/image_collections.html",
    "title": "gdalcubes",
    "section": "",
    "text": "In gdalcubes, an image collection is a set of \\(n\\) images, with data for \\(m\\) variables (or bands). Variables of the same image share a spatial footprint, recording date/time, and spatial reference system but may vary in spatial resolution. Different images may have different spatial footprints, recording date/times, and spatial reference systems.\n\n\nActual band data may come from any GDAL dataset (see https://gdal.org/user/raster_data_model.html), i.e., from anything that is readable by GDAL. This includes simple files, cloud storage, databases, and archive files through GDAL virtual file systems. Examples for readable datasets include:\n\ntest.tif (a simple local GeoTIFF file),\n/vsizip/archive.zip/xyz.tif (a GeoTIFF file in a .zip archive),\nHDF4_EOS:EOS_GRID:\"MOD13A2.A2013353.h00v08.006.2018226105756.hdf\":MODIS_Grid_16DAY_1km_VI:1 km 16 days NDVI (a subdataset in a HDF4 file from MODIS)\nSENTINEL2_L1C:S2A_OPER_MTD_SAFL1C_PDMC_20150818T101440_R022_V20150813T102406_20150813T102406.xml:10m:EPSG_32632 (a higher level GDAL Sentinel 2 dataset), or\n/vsicurl/https://download.osgeo.org/geotiff/samples/spot/chicago/UTM2GTIF.TIF (file on an HTTP server).\n/vsis3/sentinel-s2-l1c/tiles/31/U/FT/2018/5/6/0/B08.jp2 (Sentinel 2 on Amazon Web Services)\n\n\n\n\ngdalcubes implements the above definition of image collections in the following data model.\n\n\n\nFigure 1: Data model for image collections.\n\n\nThis model describes available images, available bands, and how specific GDAL datasets relate to images and bands of a collection. The data model simply points to actual data by GDAL dataset identifiers but does not store any image data. It is implemented as a relational SQLite (https://www.sqlite.org/index.html) database, i.e. can be stored as a single file, which typically consumes a few kilobytes per image. To allow for fast selection of images intersecting a given spatiotemporal area, the schema include indexes on the spatial extent and recording date/time.\ngdalcubes comes with functions to create image collections from a set of GDAL dataset identifiers. Unfortunately, it is not straightforward to extract all needed information (e.g. recording date/time) automatically from GDAL datasets. Earth observation data products are organized in extremely different ways, from single files per image to complex directory structures like the Sentinel 2 SAFE format. To let gdalcubes know, how to create image collections from a set of GDAL dataset identifiers, we must define so called image collection formats for particular data products. Formats for some Sentinel, Landsat, and MODIS products are contained in gdalcubes. The collection format definition is described in detail in the next chapter."
  },
  {
    "objectID": "source/concepts/streaming.html",
    "href": "source/concepts/streaming.html",
    "title": "gdalcubes",
    "section": "",
    "text": "Chunks of a data cube can be streamed to external processes. For example this allows to run user-defined R or Python scripts on chunks of data cubes in parallel.\nThere are currently three operations supporting chunk streaming:\n\nstream_apply_pixel streams chunks of a source data cube to an external processs and expects the same number of pixels as a result, but any number of bands.\n\nstream_reduce_time combines chunks of the same spatial part but different spans of time first, such that one resulting chunk contains complete time series of the cube and afterwards calls an external process on chunks. The external process is expected to produce a single time slice (i.e., nt = 1), the same number of pixels as the source data cube in space, and any number of bands.\nstream_chunk is a less user-friendly but more general operation that tries to derive the dimensionality of the result cube automatically, by calling the process on dummy data once.\n\nNotice that the number of result bands must be identical for all input chunks. The R package includes an easy to use implementation that allows passing R functions to apply_pixel and reduce_time.\n\n\nTo stream data to and from the external process, a custom binary serialization format is used.\nThe format includes data cube values of one chunk and includes dimension values, band names, and the spatial reference system.\n\n\n\n\n\n\nWarning\n\n\n\nThe binary serialization format will be replaced with CF compliant netCDF-4 / ZARR files in future versions.\n\n\nThe format contains\n\nthe size of the chunk as 4 * 4 bytes in total (nbands:int32, nt:int32, ny:int32, nx:int32),\nband names, where each band starts with the number of characters as int32 followed by actual characters,\ndimension values in the order time, y, x as doubles, summing to (nt + ny + nx) * 8 bytes in total,\nthe spatial reference system as a string (number of characters as int32, followed by actual characters), and\nactual values of the chunk as doubles (summing to nbands * nt * ny * nx * 8 bytes)."
  },
  {
    "objectID": "source/tutorials/bfast/bfast.html",
    "href": "source/tutorials/bfast/bfast.html",
    "title": "gdalcubes",
    "section": "",
    "text": "This tutorial shows how user-defined functions from external R packages can be applied to data cubes over time. As an example, we will use the bfast R package containing unsupervised change detection methods identifying structural breakpoints in vegetation index time series. Specifically, we will use the bfastmonitor() function to monitor changes on a time series of Sentinel-2 imagery.\n\n\n\nWe will use Sentinel-2 surface reflectance data from 2016 to 2020 covering a small forested area located southeast of Berlin. The area of interest is available as a polygon in a GeoPackage file gruenheide_forest.gpkg, which is shown in a simple map below.\n\nlibrary(sf)\n\n## Linking to GEOS 3.10.1, GDAL 3.4.0, PROJ 8.2.0; sf_use_s2() is TRUE\n\ngeom = read_sf(\"gruenheide_forest.gpkg\")\ngeom |>\n  st_bbox() -> bbox\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\n## tmap mode set to interactive viewing\n\ntm_shape(st_geometry(geom)) +  tm_polygons()\n\n\n\n\n\n\n\n\nInstead of downloading > 100 Sentinel-2 images, we use a cheap machine on Amazon Web Services (AWS) in the Oregon region, where the Sentinel-2 level 2A data are available as cloud-optimized GeoTIFFs (COGs) and explorable via the SpatioTemporal Asset Catalog (STAC) API (see here for more details about the public Sentinel-2 level 2A COG data catalog).\nUsing the rstac package, we first request all available images from 2016 to 2020 that intersect with our region of interest.\n\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n\nitems <- s |>\n    stac_search(collections = \"sentinel-s2-l2a-cogs\",\n                bbox = c(bbox[\"xmin\"],bbox[\"ymin\"],bbox[\"xmax\"],bbox[\"ymax\"]), \n                datetime = \"2016-01-01/2020-12-31\",\n                limit = 500) |>\n    post_request() \nitems\n\n## ###STACItemCollection\n## - matched feature(s): 457\n## - features (457 item(s) / 0 not fetched):\n##   - S2B_33UVU_20201229_0_L2A\n##   - S2A_33UVU_20201227_0_L2A\n##   - S2A_33UVU_20201224_0_L2A\n##   - S2B_33UVU_20201222_0_L2A\n##   - S2B_33UVU_20201219_0_L2A\n##   - S2A_33UVU_20201217_0_L2A\n##   - S2B_33UVU_20201212_0_L2A\n##   - S2B_33UVU_20201209_1_L2A\n##   - S2A_33UVU_20201207_0_L2A\n##   - S2A_33UVU_20201204_0_L2A\n##   - ... with 447 more feature(s).\n## - assets: \n## thumbnail, overview, info, metadata, visual, B01, B02, B03, B04, B05, B06, B07, B08, B8A, B09, B11, B12, AOT, WVP, SCL\n## - other field(s): \n## type, stac_version, stac_extensions, context, numberMatched, numberReturned, features, links\n\n# Date and time of first and last images\nrange(sapply(items$features, function(x) {x$properties$datetime}))\n\n## [1] \"2016-11-05T10:12:57Z\" \"2020-12-29T10:16:06Z\"\n\n\nIt turns out that 457 images intersect with our region of interest while the first available images have been recorded in November, 2016.\nTo build a regular monthly data cube, we now need to create a gdalcubes image collection from the STAC query result. Notice that to include the SCL band containing per-pixel quality flags (classification as clouds, cloud-shadows, and others), we need to explicitly list the names of the assets. We furthermore ignore images with 50% or more cloud coverage.\n\nlibrary(gdalcubes)\nassets = c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\")\nstac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 50}) -> s2_collection\n\n## Warning in stac_image_collection(items$features, asset_names = assets,\n## property_filter = function(x) {: STAC asset with name 'SCL' does not include\n## eo:bands metadata and will be considered as a single band source\n\ns2_collection\n\n## Image collection object, referencing 200 images with 12 bands\n## Images:\n##                       name     left      top   bottom    right\n## 1 S2A_33UVU_20201107_0_L2A 13.50096 53.24957 52.25346 14.89124\n## 2 S2A_33UVU_20201104_0_L2A 13.50096 53.24953 52.25346 15.14626\n## 3 S2B_33UVU_20201023_0_L2A 13.50096 53.24958 52.25346 14.90383\n## 4 S2B_33UVU_20201003_0_L2A 13.50096 53.24958 52.25346 14.90847\n## 5 S2A_33UVU_20200928_0_L2A 13.50096 53.24958 52.25346 14.90143\n## 6 S2B_33UVU_20200923_0_L2A 13.50096 53.24958 52.25346 14.91057\n##              datetime        srs\n## 1 2020-11-07T10:26:09 EPSG:32633\n## 2 2020-11-04T10:16:13 EPSG:32633\n## 3 2020-10-23T10:26:08 EPSG:32633\n## 4 2020-10-03T10:26:08 EPSG:32633\n## 5 2020-09-28T10:26:10 EPSG:32633\n## 6 2020-09-23T10:26:07 EPSG:32633\n## [ omitted 194 images ] \n## \n## Bands:\n##    name offset scale unit nodata image_count\n## 1   B01      0     1                     200\n## 2   B02      0     1                     200\n## 3   B03      0     1                     200\n## 4   B04      0     1                     200\n## 5   B05      0     1                     200\n## 6   B06      0     1                     200\n## 7   B07      0     1                     200\n## 8   B08      0     1                     200\n## 9   B09      0     1                     200\n## 10  B11      0     1                     200\n## 11  B8A      0     1                     200\n## 12  SCL      0     1                     200\n\n\nThe result contains 200 images, from which we can now create a data cube. We use the projected bounding box of our polygon as spatial extent, 10 meters spatial resolution, bilinear spatial resampling and derive monthly median values for all pixel values from multiple images within a month, if available. Notice that to make sure that the polygon is completely within our extent, we add 10m to each side of the cube.\n\nst_as_sfc(bbox) |>\n  st_transform(\"EPSG:32633\") |>\n  st_bbox() -> bbox_utm\nv = cube_view(srs = \"EPSG:32633\", extent = list(t0 = \"2016-01\", t1 = \"2020-12\", left = bbox_utm[\"xmin\"] - 10, right = bbox_utm[\"xmax\"] + 10, bottom = bbox_utm[\"ymin\"] - 10, top = bbox_utm[\"ymax\"] + 10),\n              dx = 10, dy = 10, dt = \"P1M\", aggregation = \"median\", resampling = \"bilinear\")\nv\n\n## A data cube view object\n## \n## Dimensions:\n##                low             high count pixel_size\n## t       2016-01-01       2020-12-31    60        P1M\n## y 5802003.49436843 5807403.49436843   540         10\n## x 415522.739260076 424662.739260076   914         10\n## \n## SRS: \"EPSG:32633\"\n## Temporal aggregation method: \"median\"\n## Spatial resampling method: \"bilinear\"\n\n\nNext, we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside of the polygon to NA. Afterwards we simply save the data cube as a single netCDF file. Notice that this is not needed but makes debugging to some degree easier.\n\ns2.mask = image_mask(\"SCL\", values = c(3,8,9))\ngdalcubes_options(threads = 8, ncdf_compression_level = 5)\nraster_cube(s2_collection, v, mask = s2.mask) |>\n  select_bands(c(\"B04\",\"B08\")) |>\n  filter_geom(geom$geometry) |>\n  write_ncdf(\"gruenheide_cube_monthly.nc\")\n\n\n\n\nTo get an overview of the data, we first calculate the number of available observations per time series using the built-in count reducer.\n\ngdalcubes_options(parallel = 8)\nncdf_cube(\"gruenheide_cube_monthly.nc\") |>\n  reduce_time(\"count(B04)\") |>\n  plot(key.pos = 1, zlim=c(0,60), col = viridis::viridis, nbreaks = 7)\n\n\n\n\nAlthough our collection contains 200 images with less than 50% cloud coverage, most time series only contain between 40 and 50 valid (non-masked) observations.\nWe can now use the generic function reduce_time() to apply bfastmonitor() to all kNDVI time series. Notice that there are some fully missing time series and we must carefully catch potential errors, because we do not want a single failing time series to stop all computations. The script below returns computed change dates and magnitudes for all pixel time series and writes the results to a netCDF file.\n\nncdf_cube(\"gruenheide_cube_monthly.nc\") |>\n  reduce_time(names = c(\"change_date\", \"change_magnitude\"), FUN = function(x) {\n    knr <- exp(-((x[\"B08\",]/10000)-(x[\"B04\",]/10000))^2/(2))\n    kndvi <- (1-knr) / (1+knr)   \n    if (all(is.na(kndvi))) {\n      return(c(NA,NA))\n    }\n    kndvi_ts = ts(kndvi, start = c(2016, 1), frequency = 12)\n    library(bfast)\n    tryCatch({\n        result = bfastmonitor(kndvi_ts, start = c(2020,1), level = 0.01)\n        return(c(result$breakpoint, result$magnitude))\n      }, error = function(x) {\n        return(c(NA,NA))\n      })\n  }) |>\n  write_ncdf(\"result.nc\", overwrite = TRUE)\n\nRunning bfastmonitor() is computationally expensive. However, since the data is located in the cloud anyway, it would be obvious to launch one of the more powerful machine instance types with many processors. Parallelization within one instance can be controlled entirely by gdalcubes using gdalcubes_options().\n\n\n\nTo visualize the change detection results, we load the resulting netCDF file, convert it to a stars object, and finally use the tmap package to create an interactive map to visualize the change date.\n\nlibrary(stars)\n\n## Loading required package: abind\n\nncdf_cube(\"result.nc\") |>\n  st_as_stars() -> x\ntm_shape(x[\"NETCDF:\\\"result.nc\\\":change_date\"]) + tm_raster()\n\n\n\n\n\nThe result certainly needs some postprocessing to understand types of changes and to identify false positives. The larger region in the west of the study area however clearly shows some deforestation due to the construction of Tesla’s Gigafactory Berlin-Brandenburg.\n\n\n\nThis tutorial has shown how change detection with BFAST can be applied on pixel time series of a data cube as a user-defined function. To avoid downloading a large number of images, the data cube has been created in the cloud, where Sentinel-2 level 2A imagery is already available.\nThe BFAST family of change detection methods is computationally quite expensive. For processing larger areas and/or longer time series, a more powerful machine would be helpful. However, there are quite a few ongoing developments improving the performance that may find their way to the bfast package on CRAN in the near future."
  },
  {
    "objectID": "source/tutorials/vignettes/gc02_AWS_Sentinel2.html",
    "href": "source/tutorials/vignettes/gc02_AWS_Sentinel2.html",
    "title": "2. Data cubes from Sentinel-2 data in the cloud",
    "section": "",
    "text": "This vignette will not explain any details the specifications but demonstrate how they can be used in combination with gdalcubes and the rstac package. We will use the freely available Sentinel-2 COG catalog on Amazon Web Services (AWS) and the corresponding STAC-API endpoint at https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a.\nNotice that this vignette in principle runs anywhere but computations take much longer when not working on an AWS machine in the region of the data catalog (in this case us-west-2).\n\nFinding images with rstac\nAs an example, we are interested in cloud-free images of New York City in June, 2021. We can use the NYC administrative polygons that are shipped as example data with the package and use the (sf)[https://cran.r-project.org/package=sf] package for reading and calculating the bounding box.\n\nlibrary(sf)\n## Linking to GEOS 3.10.1, GDAL 3.4.0, PROJ 8.2.0; sf_use_s2() is TRUE\nnyc_shape = read_sf(system.file(\"nycd.gpkg\",package = \"gdalcubes\"))\n\nbbox = st_bbox(nyc_shape) \nbbox\n##      xmin      ymin      xmax      ymax \n##  563069.9 4483098.0  609761.1 4529895.0\n\nTo ask the STACK endpoints for available images intersecting with our area of interest, we need to provide the bounding box in WGS84 (latitude / longitude) coordinates, which we can calculate with:\n\nnyc_shape |>\n  st_transform(\"EPSG:4326\") |>\n  st_bbox() -> bbox_wgs84\nbbox_wgs84\n##      xmin      ymin      xmax      ymax \n## -74.25559  40.49614 -73.70002  40.91500\n\nNext, we load the rstac package, specify the STAC-API endpoint URL and query all available images for our area and time of interest.\n\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n  items = s |>\n    stac_search(collections = \"sentinel-s2-l2a-cogs\",\n                bbox = c(bbox_wgs84[\"xmin\"],bbox_wgs84[\"ymin\"],\n                         bbox_wgs84[\"xmax\"],bbox_wgs84[\"ymax\"]), \n                datetime = \"2021-06-01/2021-06-30\") |>\n    post_request() |> items_fetch(progress = FALSE)\n  length(items$features)\n## [1] 48\n\nAs a result, we get a list with metadata and URLs pointing to 48 images. This list is an index-like object pointing to original images on a AWS S3 bucket and hence somewhat similar to gdalcubes image collections.\n\n\nConverting STAC items to image collections\nWe can convert the STAC response to a gdalcubes image collections using stac_image_collection(). This function expects a STAC feature list as input and optionally can apply some filters on metadata and bands. Notice that this operation is much faster than the creation of image collections from local files, because all metadata are already available and no image file must be opened. Below, we create a collection for images with less than 10% cloud cover.\n\nlibrary(gdalcubes)\ns2_collection = stac_image_collection(items$features,  property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 10})\ns2_collection\n## Image collection object, referencing 16 images with 18 bands\n## Images:\n##                       name      left      top   bottom     right\n## 1 S2B_18TWK_20210629_0_L2A -75.00023 40.65085 39.65842 -73.78649\n## 2 S2B_18TXK_20210629_0_L2A -73.81752 40.64478 40.55838 -73.78649\n## 3 S2B_18TWL_20210629_0_L2A -75.00023 41.55184 40.55671 -73.68381\n## 4 S2B_18TXL_20210629_0_L2A -73.81884 41.54559 40.55671 -73.45793\n## 5 S2A_18TWK_20210624_0_L2A -75.00023 40.65085 39.65836 -73.77774\n## 6 S2A_18TXK_20210624_0_L2A -73.81796 40.64478 40.53306 -73.77774\n##              datetime        srs\n## 1 2021-06-29T16:02:02 EPSG:32618\n## 2 2021-06-29T16:01:52 EPSG:32618\n## 3 2021-06-29T16:01:48 EPSG:32618\n## 4 2021-06-29T16:01:42 EPSG:32618\n## 5 2021-06-24T16:02:02 EPSG:32618\n## 6 2021-06-24T16:01:52 EPSG:32618\n## [ omitted 10 images ] \n## \n## Bands:\n##            name offset scale unit nodata image_count\n## 1           B01      0     1                      16\n## 2           B02      0     1                      16\n## 3           B03      0     1                      16\n## 4           B04      0     1                      16\n## 5           B05      0     1                      16\n## 6           B06      0     1                      16\n## 7           B07      0     1                      16\n## 8           B08      0     1                      16\n## 9           B09      0     1                      16\n## 10          B11      0     1                      16\n## 11          B12      0     1                      16\n## 12          B8A      0     1                      16\n## 13 overview:B02      0     1                      16\n## 14 overview:B03      0     1                      16\n## 15 overview:B04      0     1                      16\n## 16   visual:B02      0     1                      16\n## 17   visual:B03      0     1                      16\n## 18   visual:B04      0     1                      16\n\nThe collection contains 16 images and all spectral bands plus some RGB preview images. However, the scene classification layer (SCL) containing pixel-wise information whether a pixel shows a cloud, cloud-shadow, water, or something else, is missing. Although the SCL images are available in the returned STAC list, the response does not include all of the metadata to let gdalcubes recognize it as an image. However, it is possible to explicitly list the names of all bands to be included in the collection by adding the asset_names argument:\n\nassets = c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\")\ns2_collection = stac_image_collection(items$features, asset_names = assets, property_filter =\n                                      function(x) {x[[\"eo:cloud_cover\"]] < 10})\ns2_collection\n## Image collection object, referencing 16 images with 12 bands\n## Images:\n##                       name      left      top   bottom     right\n## 1 S2B_18TWK_20210629_0_L2A -75.00023 40.65085 39.65842 -73.78649\n## 2 S2B_18TXK_20210629_0_L2A -73.81752 40.64478 40.55838 -73.78649\n## 3 S2B_18TWL_20210629_0_L2A -75.00023 41.55184 40.55671 -73.68381\n## 4 S2B_18TXL_20210629_0_L2A -73.81884 41.54559 40.55671 -73.45793\n## 5 S2A_18TWK_20210624_0_L2A -75.00023 40.65085 39.65836 -73.77774\n## 6 S2A_18TXK_20210624_0_L2A -73.81796 40.64478 40.53306 -73.77774\n##              datetime        srs\n## 1 2021-06-29T16:02:02 EPSG:32618\n## 2 2021-06-29T16:01:52 EPSG:32618\n## 3 2021-06-29T16:01:48 EPSG:32618\n## 4 2021-06-29T16:01:42 EPSG:32618\n## 5 2021-06-24T16:02:02 EPSG:32618\n## 6 2021-06-24T16:01:52 EPSG:32618\n## [ omitted 10 images ] \n## \n## Bands:\n##    name offset scale unit nodata image_count\n## 1   B01      0     1                      16\n## 2   B02      0     1                      16\n## 3   B03      0     1                      16\n## 4   B04      0     1                      16\n## 5   B05      0     1                      16\n## 6   B06      0     1                      16\n## 7   B07      0     1                      16\n## 8   B08      0     1                      16\n## 9   B09      0     1                      16\n## 10  B11      0     1                      16\n## 11  B8A      0     1                      16\n## 12  SCL      0     1                      16\n\n\n\nCreating and processing data cubes\nHaving an image collection object, we can now use gdalcubes in the same way as we do with local files. Particularly, we define a data cube view and maybe some additional data cube operations. In the following example, we create a coarse resolution (100m) simple median-composite RGB image from a daily data cube.\n\ngdalcubes_options(parallel = 8)\nv = cube_view(srs=\"EPSG:32618\", dx=100, dy=100, dt=\"P1D\", \n                           aggregation=\"median\", resampling = \"average\",\n                           extent=list(t0 = \"2021-06-01\", t1 = \"2021-06-30\",\n                                       left=bbox[\"xmin\"], right=bbox[\"xmax\"],\n                                       top=bbox[\"ymax\"], bottom=bbox[\"ymin\"]))\nv\n## A data cube view object\n## \n## Dimensions:\n##                low             high count pixel_size\n## t       2021-06-01       2021-06-30    30        P1D\n## y 4483096.51932691 4529896.51932691   468        100\n## x 563065.499607774 609765.499607774   467        100\n## \n## SRS: \"EPSG:32618\"\n## Temporal aggregation method: \"median\"\n## Spatial resampling method: \"average\"\n\nraster_cube(s2_collection, v) |>\n  select_bands(c(\"B02\",\"B03\",\"B04\")) |>\n  reduce_time(c(\"median(B02)\", \"median(B03)\", \"median(B04)\")) |>\n  plot(rgb = 3:1, zlim = c(0,2500))\n\n\n\n\n\n\n\n\nOf course, we can “zoom in” to Lower Manhattan simply by changing the data cube view.\n\nv = cube_view(srs=\"EPSG:32618\", dx=10, dy=10, dt=\"P1D\", \n                           aggregation=\"median\", resampling = \"average\",\n                           extent=list(t0 = \"2021-06-01\", t1 = \"2021-06-30\",\n                                       left=582182, right=587019,\n                                       top=4508997, bottom=4505883))\nv\n## A data cube view object\n## \n## Dimensions:\n##          low       high count pixel_size\n## t 2021-06-01 2021-06-30    30        P1D\n## y    4505880    4509000   312         10\n## x   582180.5   587020.5   484         10\n## \n## SRS: \"EPSG:32618\"\n## Temporal aggregation method: \"median\"\n## Spatial resampling method: \"average\"\nraster_cube(s2_collection, v) |>\n  select_bands(c(\"B02\",\"B03\",\"B04\")) |>\n  reduce_time(c(\"median(B02)\", \"median(B03)\", \"median(B04)\")) |>\n  plot(rgb = 3:1, zlim = c(0,2500))\n\n\n\n\n\n\n\n\nFor more complex examples, you can find a tutorial on YouTube and corresponding materials on GitHub how to use gdalcubes in the cloud, presented at the virtual OpenGeoHub Summer School 2021.\n\n\nSummary\nThanks to STAC, cloud-optimized GeoTIFFs, and GDAL being capable of reading imagery from cloud storage directly, moving to cloud-based analysis workflows without downloading any imagery from portals become a lot easier and more efficient. Whether or not this is the right approach depends a lot on particular applications. In some cases (e.g. when having access to institutional HPC resources while applying very complex models on small areas of interest), it might still be appropriate to download the data once. The gdalcubes package still can help as an interface to downloading analysis-ready data cubes instead of image files from portals."
  },
  {
    "objectID": "source/tutorials/vignettes/gc01_MODIS.html",
    "href": "source/tutorials/vignettes/gc01_MODIS.html",
    "title": "1. Creating data cubes from local MODIS imagery",
    "section": "",
    "text": "The gdalcubes package aims at making the work with large collections of Earth observation (EO) imagery (e.g. from Sentinel 2) easier and faster. Typical challenges with these data such as overlapping images, different spatial resolutions of spectral bands, irregular temporal sampling, and different coordinate reference systems are abstracted away from users by reading the data as a raster data cube and letting users define the shape of the cube (spatiotemporal extent, resolution, and spatial reference system). Working with EO imagery then becomes more interactive: going from “try method X on low resolution and get the result asap” to “apply the final method to the full resolution dataset over night” becomes straightforward.\nThis brief vignette illustrates basic ideas of the package. We will use satellite imagery from the Moderate Resolution Imaging Spectroradiometer (MODIS) that is small enough to process even on older machines. The imagery comes as a set of HDF4 files. We assume that you have successfully installed the gdalcubes package. Please also make sure that your GDAL installation supports the HDF4 driver (e.g. with gdalcubes_gdalformats()).\nIn the following, we will follow a simple workflow by"
  },
  {
    "objectID": "source/tutorials/vignettes/gc01_MODIS.html#aggregation-and-resampling",
    "href": "source/tutorials/vignettes/gc01_MODIS.html#aggregation-and-resampling",
    "title": "1. Creating data cubes from local MODIS imagery",
    "section": "Aggregation and resampling",
    "text": "Aggregation and resampling\nBesides the spatiotemporal extent, the resolution and the spatial reference system, the data cube view contains the two important parameters aggregation and resampling. Resampling here refers to how images are resampled in space during the reprojection, scaling, and cropping operations. The temporal aggregation method defines how values for the same cell from different images are combined in the target cube. For example, a data cube with monthly temporal resolution will contain values from multiple images. Currently, possible values are first, last, min, max, mean, and median. These functions are evaluated per data cube pixel."
  },
  {
    "objectID": "source/tutorials/vignettes/gc01_MODIS.html#export-data-cubes-to-files",
    "href": "source/tutorials/vignettes/gc01_MODIS.html#export-data-cubes-to-files",
    "title": "1. Creating data cubes from local MODIS imagery",
    "section": "Export data cubes to files",
    "text": "Export data cubes to files\nReplacing the call to plot() with write_ncdf(), or write_tif() would write the result as NetCDF or GeoTIFF files to disk. While write_ncdf() always produces a single file, write_tif() produces one file per time slice and the time is automatically added to the provided output filename. Both functions support compression and modifying the data type to save disk space."
  },
  {
    "objectID": "source/tutorials/vignettes/gc01_MODIS.html#parallel-processing",
    "href": "source/tutorials/vignettes/gc01_MODIS.html#parallel-processing",
    "title": "1. Creating data cubes from local MODIS imagery",
    "section": "Parallel processing",
    "text": "Parallel processing\ngdalcubes supports parallel processing of data cube operations. Calling gdalcubes_options(parallel = n) will tell all following data cube operations to use up to n parallel processes. Notice that worker processes are assigned to chunks of the data cube and the true number of processes can be lower if there are less than n chunks."
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html",
    "title": "3. Extracting training data for machine learning models",
    "section": "",
    "text": "Machine learning models for land cover classification, change detection, spatiotemporal prediction, and similar tasks in most cases need a large number of observations for training.\nThis vignette will demonstrate how training data from satellite image collections can be extracted for typical tasks including:\nOne function that can do all of this is extract_geom(), which is similar to the st_extract() function from the stars package and the extract() function from the raster and terra packages."
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html#the-extract_geom-function",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html#the-extract_geom-function",
    "title": "3. Extracting training data for machine learning models",
    "section": "The extract_geom() function",
    "text": "The extract_geom() function\nGiven a data cube and any simple feature geometries as an sf object, the extract_geom() function can be used as a general method to extract data cube pixel values at irregular locations. extract_geom() returns a data.frame with columns for feature identifiers (FIDs, often row numbers of the sf object), time, and bands / variables of the data cube. Each row represents the data cube values of one pixel relating to the feature given by the FID column. For anything other than simple point geometries (e.g. POLYGON, LINESTRING, MULTIPOINT, and similar), the result may contain multiple rows per feature. In these cases, it is possible to apply an aggregation function to compute mean, median or similar summary statistics over features.\nextract_geom() drops any pixels with missing values only. Hence, if a feature is outside the extent of the data cube, or all pixels of a feature are NA due to clouds or unavailability of images, these pixels will not be included in the result. In contrast, if the input features contain overlapping geometries, pixels may be included several times (with different values in the FID column).\nFor multitemporal cubes, the full time series of pixels relating to the features is returned by default, leading to multiple rows with different time values. It is possible to specify the date/time of features using either an available time column from the sf object by name (argument time_column), or as an additional Date, POSIXt, or character vector with length corresponding to the number of features (argument datetime). In this case, only data cube pixels related to the time value of features is returned in the result instead of the full time series.\nCompared to the extract() function from the raster and terra packages, extract_geom() is a little less flexible. For example, it is not possible to derive fractions of pixels covered by the features to compute weighted aggregations or similar."
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html#extracting-pixel-values-and-summary-statistics-from-spatial-polygons",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html#extracting-pixel-values-and-summary-statistics-from-spatial-polygons",
    "title": "3. Extracting training data for machine learning models",
    "section": "1. Extracting pixel values and summary statistics from spatial polygons",
    "text": "1. Extracting pixel values and summary statistics from spatial polygons\nAs a small example how to prepare training data for simple classification tasks, we use a labeled land cover polygon dataset covering the city of Muenster, Germany, which can be downloaded from https://uni-muenster.sciebo.de/s/fgyaomOJxSd93H8/download. This GeoPackage dataset contains spatial polygons with a column class representing land cover. We can directly download and plot the features using the sf package:\n\nlibrary(sf)\n## Linking to GEOS 3.10.1, GDAL 3.4.0, PROJ 8.2.0; sf_use_s2() is TRUE\ntraining_sites = read_sf(\"https://uni-muenster.sciebo.de/s/fgyaomOJxSd93H8/download\")\nplot(training_sites, axes = TRUE, key.pos = 1)\n\n\n\n\n\n\n\n\nThis is a rather small toy dataset but since the features are polygons, they already cover quite a few 10m pixels from Sentinel-2 imagery. As a first step to extract Sentinel-2 values within the polygons, we create a (virtual) data cube from Sentinel-2 data on Amazon Web Services (see previous vignette). Since the data is openly available, we can still work locally and do not need to run a machine on AWS (though this would be much faster for larger polygon datasets).\nAs our area of interest, we use the extent of the polygon dataset and look for (cloud-free) observations in July, 2021. To find corresponding images, we use the rstac package and query from the Sentinel-2 cloud-optimized GeoTIFF collection on AWS.\n\nbbox = st_bbox(training_sites) \nbbox\n##      xmin      ymin      xmax      ymax \n##  7.576647 51.874603  7.673467 51.977592\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n  items = s |>\n    stac_search(collections = \"sentinel-s2-l2a-cogs\",\n                bbox = c(bbox[\"xmin\"],bbox[\"ymin\"],\n                         bbox[\"xmax\"],bbox[\"ymax\"]), \n                datetime = \"2021-07-01/2021-07-31\") |>\n    post_request() |> items_fetch(progress = FALSE)\n  length(items$features)\n## [1] 26\n\nTo filter by cloud coverage and create a gdalcubes image collection object, we apply stac_image_collection() on the resulting list of 26 images.\n\nlibrary(gdalcubes)\ns2_collection = stac_image_collection(items$features, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 20})\ns2_collection\n## Image collection object, referencing 5 images with 18 bands\n## Images:\n##                       name     left      top   bottom    right\n## 1 S2B_32ULC_20210723_0_L2A 6.946499 52.34304 51.34525 7.704564\n## 2 S2B_32UMC_20210723_0_L2A 7.531544 52.35038 51.35444 9.143276\n## 3 S2A_32ULC_20210718_0_L2A 6.952812 52.34304 51.34536 7.704564\n## 4 S2A_32UMC_20210718_0_L2A 7.531544 52.35038 51.35444 9.143276\n## 5 S2B_32ULC_20210703_0_L2A 6.948221 52.34304 51.34528 7.704564\n##              datetime        srs\n## 1 2021-07-23T10:36:40 EPSG:32632\n## 2 2021-07-23T10:36:36 EPSG:32632\n## 3 2021-07-18T10:36:41 EPSG:32632\n## 4 2021-07-18T10:36:37 EPSG:32632\n## 5 2021-07-03T10:36:39 EPSG:32632\n## \n## Bands:\n##            name offset scale unit nodata image_count\n## 1           B01      0     1                       5\n## 2           B02      0     1                       5\n## 3           B03      0     1                       5\n## 4           B04      0     1                       5\n## 5           B05      0     1                       5\n## 6           B06      0     1                       5\n## 7           B07      0     1                       5\n## 8           B08      0     1                       5\n## 9           B09      0     1                       5\n## 10          B11      0     1                       5\n## 11          B12      0     1                       5\n## 12          B8A      0     1                       5\n## 13 overview:B02      0     1                       5\n## 14 overview:B03      0     1                       5\n## 15 overview:B04      0     1                       5\n## 16   visual:B02      0     1                       5\n## 17   visual:B03      0     1                       5\n## 18   visual:B04      0     1                       5\n\nThe collection contains five images only. However, we now create a rather large data cube with spatial extent from the image collection and 10m spatial resolution. Notice that this data cube is not downloaded but only created virtually, as a proxy object that knows where the corresponding images are located and what to do with the data when needed. In the example below, we use the visible RGB and the near infrared bands and add the NDVI vegetation index as a data cube band. Notice that we do not use a per-pixel cloud mask here.\n\nv = cube_view(extent=s2_collection, dt=\"P1M\", dx=10, dy=10, srs=\"EPSG:3857\", \n                      aggregation = \"median\", resampling = \"bilinear\")\n\nraster_cube(s2_collection, v) |> # no mask\n  select_bands(c(\"B02\",\"B03\",\"B04\",\"B08\")) |>\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\", keep_bands = TRUE) -> ms_cube\nms_cube\n## A GDAL data cube proxy object\n## \n## Dimensions:\n##                low             high count pixel_size chunk_size\n## t       2021-07-01       2021-07-31     1        P1M          1\n## y 6682590.54960759 6863730.54960759 18114         10       1024\n## x 773277.779989172 1017827.77998917 24455         10       1024\n## \n## Bands:\n##   name offset scale nodata unit\n## 1  B02      0     1    NaN     \n## 2  B03      0     1    NaN     \n## 3  B04      0     1    NaN     \n## 4  B08      0     1    NaN     \n## 5 NDVI      0     1    NaN\n\nThe cube has 24455 x 18114 spatial pixels, which would sum to a GeoTIFF file of several gigabytes (depending on data type and compression), although the area of interest is quite small and we are only interested in a few pixels in the polygons. Fortunately, extract_geom() reduces unnecessary data reads to a large extent, meaning that even if we would use a data cube for whole Germany at 10m resolution, it would only read blocks of the data covering our area of interest, and simply ignore other parts.\n\nx = extract_geom(ms_cube, training_sites)\nnrow(x)\n## [1] 12744\nhead(x)\n##   FID       time      B02      B03      B04      B08      NDVI\n## 1   7 2021-07-01 312.3927 454.4502 300.3140 4142.701 0.8648152\n## 2   7 2021-07-01 314.5155 445.5853 303.2982 4160.428 0.8641054\n## 3   7 2021-07-01 320.6820 445.7498 307.1665 4165.566 0.8626493\n## 4   7 2021-07-01 313.4379 459.7528 307.8308 4106.122 0.8605192\n## 5   7 2021-07-01 318.1167 446.6952 311.6203 4140.536 0.8600138\n## 6   7 2021-07-01 324.3806 444.3545 312.0102 4149.988 0.8601478\n\nAs expected, the result contains multiple rows per polygon (because polygons cover multiple pixels). To compute summary statistics per polygon, we can provide a function as the FUN argument:\n\nx = extract_geom(ms_cube, training_sites, FUN = median)\n#x = extract_geom(ms_cube, training_sites)#), FUN = median)\nx\n##    FID       time       B02       B03       B04       B08        NDVI\n## 1    1 2021-07-01  266.2039  389.5117  218.2514  237.6531  0.04524603\n## 2    2 2021-07-01  266.8352  368.6943  223.0690  257.1879  0.07006745\n## 3    3 2021-07-01  642.4923  884.2024  987.9066 2134.6692  0.36731547\n## 4    4 2021-07-01  339.1859  519.5446  314.6939 4237.5824  0.86212147\n## 5    5 2021-07-01  742.0260 1048.1800 1432.2655 2591.4463  0.28675374\n## 6    6 2021-07-01  554.2089  829.8890  866.6759 2255.3755  0.44919218\n## 7    7 2021-07-01  427.3777  596.9655  485.9907 3970.5424  0.78361568\n## 8    8 2021-07-01 1181.4101 1277.0147 1335.0851 1752.2424  0.10188053\n## 9    9 2021-07-01  858.2257  990.5626 1143.5540 1573.5704  0.14296265\n## 10  10 2021-07-01 1361.9271 1536.8561 1653.4901 2186.0671  0.09075880\n## 11  11 2021-07-01  721.8104  867.5908 1001.1942 1868.7402  0.25241746\n## 12  12 2021-07-01  269.9440  405.1574  237.6765 3493.1907  0.87339904\n## 13  13 2021-07-01  288.0669  424.8431  261.3002 3458.0253  0.86101114\n## 14  14 2021-07-01  277.8512  412.7048  245.6007 4087.6796  0.88691074\n## 15  15 2021-07-01  599.5534  821.2706  498.7003  260.6612 -0.30120940\n## 16  16 2021-07-01  538.8088  758.5329  407.5442  311.8426 -0.15754052\n\nTo combine the extracted data cube values with the original sf objects including the geometries, the merge() function can be used. merge() performs table join operations on common columns (e.g. IDs). We therefore first need to add an FID column to the features and then join both tables by their FID columns. Notice that by default, this is performing an inner join, i.e. rows with FIDs that only exist in one table will be dropped. Alternatively, we can set all.x=TRUE to make sure that our result contains all features from the original dataset (left outer join). Below, we combine the tables, drop the geometries and order by NDVI, showing a clear relation between class and NDVI.\n\nx = x[order(x$NDVI,decreasing = T),]\ntraining_sites$FID = rownames(training_sites)\ntrn_df = sf::st_drop_geometry(merge(training_sites, x, by = \"FID\"))\ntrn_df[order(trn_df$NDVI, decreasing = TRUE),]\n##    FID       class       time       B02       B03       B04       B08\n## 6   14      forest 2021-07-01  277.8512  412.7048  245.6007 4087.6796\n## 4   12      forest 2021-07-01  269.9440  405.1574  237.6765 3493.1907\n## 11   4 agriculture 2021-07-01  339.1859  519.5446  314.6939 4237.5824\n## 5   13      forest 2021-07-01  288.0669  424.8431  261.3002 3458.0253\n## 14   7 agriculture 2021-07-01  427.3777  596.9655  485.9907 3970.5424\n## 13   6 agriculture 2021-07-01  554.2089  829.8890  866.6759 2255.3755\n## 10   3 agriculture 2021-07-01  642.4923  884.2024  987.9066 2134.6692\n## 12   5 agriculture 2021-07-01  742.0260 1048.1800 1432.2655 2591.4463\n## 3   11       urban 2021-07-01  721.8104  867.5908 1001.1942 1868.7402\n## 16   9       urban 2021-07-01  858.2257  990.5626 1143.5540 1573.5704\n## 15   8       urban 2021-07-01 1181.4101 1277.0147 1335.0851 1752.2424\n## 2   10       urban 2021-07-01 1361.9271 1536.8561 1653.4901 2186.0671\n## 9    2       water 2021-07-01  266.8352  368.6943  223.0690  257.1879\n## 1    1       water 2021-07-01  266.2039  389.5117  218.2514  237.6531\n## 8   16       water 2021-07-01  538.8088  758.5329  407.5442  311.8426\n## 7   15       water 2021-07-01  599.5534  821.2706  498.7003  260.6612\n##           NDVI\n## 6   0.88691074\n## 4   0.87339904\n## 11  0.86212147\n## 5   0.86101114\n## 14  0.78361568\n## 13  0.44919218\n## 10  0.36731547\n## 12  0.28675374\n## 3   0.25241746\n## 16  0.14296265\n## 15  0.10188053\n## 2   0.09075880\n## 9   0.07006745\n## 1   0.04524603\n## 8  -0.15754052\n## 7  -0.30120940"
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html#time-series-extraction-from-spatial-points",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html#time-series-extraction-from-spatial-points",
    "title": "3. Extracting training data for machine learning models",
    "section": "2. Time series extraction from spatial points",
    "text": "2. Time series extraction from spatial points\nIn the next example, we use MODIS land surface temperature measurements over Europe (see first vignette) and extract time series in London, Paris, and Barcelona. For each city, we define some points in the urban center as well as in the rural surrounding areas.\nWe start with downloading the MODIS data, if needed:\n\ndest_dir = tempdir()\nif (!dir.exists(file.path(dest_dir,\"MOD11A2\"))) {\n  options(timeout = max(1800, getOption(\"timeout\")))\n  download.file(\"https://uni-muenster.sciebo.de/s/eP9E6OIkQbXrmsY/download\", destfile=file.path(dest_dir, \"MOD11A2.zip\"),mode = \"wb\")\n  unzip(file.path(dest_dir, \"MOD11A2.zip\"), exdir = file.path(dest_dir,\"MOD11A2\"))\n  unlink(file.path(dest_dir, \"MOD11A2.zip\"))\n}\n\nNext, we build a gdalcubes image collection object:\n\nlibrary(gdalcubes)\nfiles = list.files(file.path(dest_dir,\"MOD11A2\"), pattern=\".hdf$\", full.names = TRUE)\nMODIS.collection = create_image_collection(files, \"MxD11A2\")\nMODIS.collection\n## Image collection object, referencing 140 images with 8 bands\n## Images:\n##                                                                name      left\n## 1 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018001.h17v03.006.2018011145329 -20.00000\n## 2 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018001.h17v04.006.2018011145438 -15.55724\n## 3 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018001.h18v03.006.2018011145428   0.00000\n## 4 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018001.h18v04.006.2018011145326   0.00000\n## 5 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018009.h17v03.006.2018018034330 -20.00000\n## 6 /tmp/Rtmps3iUD7/MOD11A2/MOD11A2.A2018009.h17v04.006.2018018034246 -15.55724\n##   top bottom    right            datetime\n## 1  60     50  0.00000 2018-01-01T00:00:00\n## 2  50     40  0.00000 2018-01-01T00:00:00\n## 3  60     50 20.00000 2018-01-01T00:00:00\n## 4  50     40 15.55724 2018-01-01T00:00:00\n## 5  60     50  0.00000 2018-01-09T00:00:00\n## 6  50     40  0.00000 2018-01-09T00:00:00\n##                                                                                                                                                                                                                                                                                                                                                                                                                                        srs\n## 1 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## 2 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## 3 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## 4 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## 5 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## 6 PROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not specified (based on custom spheroid)\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n## [ omitted 134 images ] \n## \n## Bands:\n##              name offset scale unit     nodata image_count\n## 1   DAY_VIEW_TIME   0.00 0.100  hrs 255.000000         140\n## 2         EMIS_31   0.49 0.002        0.000000         140\n## 3         EMIS_32   0.49 0.002        0.000000         140\n## 4         LST_DAY   0.00 0.020    K   0.000000         140\n## 5       LST_NIGHT   0.00 0.020    K   0.000000         140\n## 6 NIGHT_VIEW_TIME   0.00 0.100  hrs 255.000000         140\n## 7          QC_DAY   0.00 1.000                         140\n## 8        QC_NIGHT   0.00 1.000                         140\n\nNow, we create some sample points from hand, where we want to extract the time series of land surface temperature measurements. We convert the created data.frame to an sf object using st_as_sf().\n\n# create points from hand...\nx = c(2.347821, 2.3062300, 2.3802715, 2.3562110, 2.473618884, 2.2717150, 1.9978976)\ny = c(48.853590, 48.8422630, 48.8680197, 48.8901057, 48.670428823, 49.0335277, 48.6987311)\nname = c(\"PARIS_URBAN_1\", \"PARIS_URBAN_2\", \"PARIS_URBAN_3\", \"PARIS_URBAN_4\", \"PARIS_RURAL_1\", \"PARIS_RURAL_2\", \"PARIS_RURAL_3\")\n\nx = c(x, -0.1004895, -0.1018785, -0.1250968, -0.0810867, 0.0490169, -0.461243207, -0.2806675, -0.3103141)\ny = c(y, 51.4941646, 51.4653369, 51.5268144, 51.5109185, 51.6569130, 51.589319769, 51.2611309, 51.6595132)\nname = c(name, \"LONDON_URBAN_1\", \"LONDON_URBAN_2\", \"LONDON_URBAN_3\",\"LONDON_URBAN_4\", \"LONDON_RURAL_1\", \"LONDON_RURAL_2\", \"LONDON_RURAL_3\", \"LONDON_RURAL_4\")\n\nx = c(x,2.1599154, 2.19904748, 2.2230235, 2.1670374, 2.2290286, 1.9649098)\ny = c(y, 41.3879580, 41.42672217, 41.4274755, 41.4556412, 41.4823003, 41.3235823)\nname = c(name, \"BARCELONA_URBAN_1\", \"BARCELONA_URBAN_2\", \"BARCELONA_URBAN_3\", \"BARCELONA_RURAL_1\", \"BARCELONA_RURAL_2\", \"BARCELONA_RURAL_3\")\n\npts = data.frame(x = x, y = y, name = name)\n\nlibrary(sf)\nsf = st_as_sf(pts, coords = c(\"x\",\"y\"), crs = st_crs(4326))\n\nIn the next step, we build a 1km 8-daily cube over Europe, convert the measurements to degree Celsius and extract the time series using the extract_geom() function.\n\ngdalcubes_options(parallel = 8)\nv = cube_view(extent=MODIS.collection, srs = \"EPSG:3035\", dx = 1000, dy = 1000, dt = \"P8D\")\nraster_cube(MODIS.collection, v)  |>\n  select_bands(c(\"LST_DAY\")) |>\n  apply_pixel(\"LST_DAY * 0.02 - 273.15\", \"LST\") |>\n  extract_geom(sf) -> result\nhead(result, n = 40)\n##    FID       time   LST\n## 1   16 2018-01-01 13.33\n## 2   17 2018-01-01 13.63\n## 3   18 2018-01-01 13.33\n## 4   19 2018-01-01 12.15\n## 5   20 2018-01-01 14.73\n## 6   21 2018-01-01 15.91\n## 7   16 2018-01-09 12.23\n## 8   17 2018-01-09 12.15\n## 9   18 2018-01-09 12.07\n## 10  19 2018-01-09 10.19\n## 11  20 2018-01-09  9.83\n## 12  21 2018-01-09  9.81\n## 13  16 2018-01-17 17.13\n## 14  17 2018-01-17 16.87\n## 15  18 2018-01-17 16.03\n## 16  19 2018-01-17 14.03\n## 17  20 2018-01-17 13.65\n## 18  21 2018-01-17 13.81\n## 19   1 2018-01-09  6.83\n## 20   2 2018-01-09  6.29\n## 21   3 2018-01-09  7.37\n## 22   4 2018-01-09  7.67\n## 23   5 2018-01-09  5.57\n## 24   6 2018-01-09  0.79\n## 25   7 2018-01-09  6.33\n## 26  16 2018-02-02  9.21\n## 27  17 2018-02-02  5.43\n## 28  18 2018-02-02  7.33\n## 29  19 2018-02-02  7.39\n## 30  20 2018-02-02  8.41\n## 31  21 2018-02-02  7.15\n## 32  16 2018-01-25 15.11\n## 33  17 2018-01-25 14.85\n## 34  18 2018-01-25 15.11\n## 35  19 2018-01-25 13.51\n## 36  20 2018-01-25 12.91\n## 37  21 2018-01-25 11.67\n## 38  16 2018-02-18 14.05\n## 39  17 2018-02-18 15.11\n## 40  18 2018-02-18 16.17\n\nThe result contains FID, time, and LST as columns and we can combine the data with the original sf object with merge():\n\nsf$FID = rownames(sf)\ndf = merge(sf, result, by = \"FID\")\n\nWe can plot the time series as in the example below, showing some differences between the rural and the urban locations for Paris and Barcelona.\n\nlibrary(ggplot2)\n\ndf |>\n  dplyr::filter(startsWith(name, \"PARIS\")) |>\n  ggplot( aes(x = as.Date(time), y = LST, color = FID, group = FID)) +\n  geom_line() + geom_point() + ggtitle(\"Paris\") + xlab(\"Time\") + ylab(\"LST [K]\")\n\n\n\n\n\n\n\n  \ndf |>\n  dplyr::filter(startsWith(name, \"BARCELONA\")) |>\n  ggplot( aes(x = as.Date(time), y = LST, color = FID, group = FID)) +\n  geom_line() + geom_point()  + ggtitle(\"Barcelona\") + xlab(\"Time\") + ylab(\"LST [K]\")\n\n\n\n\n\n\n\n\nA similar workflow can e.g. be used to create patterns for time series classification using the dtwSat package."
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html#combining-satellite-observations-with-in-situ-observations",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html#combining-satellite-observations-with-in-situ-observations",
    "title": "3. Extracting training data for machine learning models",
    "section": "3. Combining satellite observations with in-situ observations",
    "text": "3. Combining satellite observations with in-situ observations\nThe previous examples have used spatial features without time information. For applications where interest lies in the combination of satellite-based and in-situ observations or adding satellite observations to movement data such as animal tracks, time information is available for the spatial features (mostly points) and should be considered in the analysis. In this example, we will use primary data from the European Land Use and Coverage Area frame Survey (LUCAS) containing land cover point samples from 2018. The data can be downloaded as country-wise CSV files (see here). We will use observations over Germany, subset some specific crop types and combine the points with Sentinel-2 NDVI observations.\nAs a first step, we load the data, remove rows with missing coordinates and convert the data.frame to an sf object. ::: {.cell}\nx = read.csv(\"https://ec.europa.eu/eurostat/cache/lucas/DE_2018_20200213.CSV\")\nx = x[-which(is.na(x$TH_LAT) | is.na(x$TH_LONG)),]\nx = st_as_sf(x, coords = c(\"TH_LONG\", \"TH_LAT\"), crs = \"EPSG:4326\")\n:::\nAfterwards we extract the observation date from points and add it as a new column t.\n\nx$t = as.Date(x$SURVEY_DATE, format = \"%d/%m/%y\") \n\nThe dominant land cover type is encoded in the LC1 column, where a letter indicates the major class (e.g., forest, agriculture, water, or urban) that is followed by a number to identify more specific types (e.g. wheat, barley, oat, and others for class agriculture). Here, we are interested in four different crop types (“common wheat”, “barley”, “rye”, and “maize”). Below, we select corresponding points and randomly sample 100 points for each type and add common names to the relevant land cover types.\n\nx[startsWith(x$LC1, c(\"B11\", \"B13\", \"B14\", \"B16\")), c(\"LC1\",\"t\")] |>\n  dplyr::group_by(LC1) |>\n  dplyr::slice_sample(n = 100) -> training\n\nnames = c(\"B11\" = \"Common Wheat\",\n          \"B13\" = \"Barley\",\n          \"B14\" = \"Rye\",\n          \"B16\" = \"Maize\")\n\ntraining$LC1_NAME = names[training$LC1]\ntable(training$LC1_NAME)\n## \n##       Barley Common Wheat        Maize          Rye \n##          100          100          100          100\nplot(training[,\"LC1_NAME\"], key.pos = 1)\n\n\n\n\n\n\n\n\nThe result contains 400 samples with observations at different dates. To add NDVI measurements to the data, we now request available Sentinel-2 images from the Sentinel-2 cloud-optimized GeoTIFF collection on AWS within the area and time of interest using rstac.\n\nbbox = st_bbox(training) \nbbox\n##      xmin      ymin      xmax      ymax \n##  6.223914 47.753108 14.595403 54.885928\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\nitems = s |>\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = c(bbox[\"xmin\"],bbox[\"ymin\"],\n                       bbox[\"xmax\"],bbox[\"ymax\"]), \n              datetime = \"2018-04-01/2018-12-31\") |>\n  post_request() |> items_fetch(progress = FALSE)\n\nThe result contains 9486 images. Next, we load the gdalcubes package, create an image collection object from the STAC result and at the same time filter images by cloud cover.\n\nlibrary(gdalcubes)\ns2_collection = stac_image_collection(items$features, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 10})\n\nThe collection contains 2387 remaining images with less than 10% cloud cover. Now we create a data cube at original 10m spatial resolution and use the revisit time (5 days) as temporal resolution. We also derive NDVI measurements and drop unneeded bands on-the-fly.\n\nv = cube_view(extent=s2_collection, dt=\"P5D\", dx=10, dy=10, srs=\"EPSG:3857\", \n              aggregation = \"median\", resampling = \"nearest\")\n\nraster_cube(s2_collection, v, chunking = c(1,512,512)) |> \n  select_bands(c(\"B04\",\"B08\")) |>\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") -> cube\ncube\n## A GDAL data cube proxy object\n## \n## Dimensions:\n##                low             high  count pixel_size chunk_size\n## t       2018-04-01       2018-12-31     55        P5D          1\n## y  5908460.4569991  7370990.4569991 146253         10        512\n## x 503996.201529035 1686796.20152904 118280         10        512\n## \n## Bands:\n##   name offset scale nodata unit\n## 1 NDVI      0     1    NaN\n\nNotice that the selection of resolution here has some important implications:\n\nSpatially larger pixels increase the likelihood that the pixel corresponding with a point contains different land cover classes, a moderate increasement, however, may still be helpful to include a bit of spatial context and reduce noise.\nUsing a lower temporal resolution (e.g. in a monthly aggregated cube) reduces the amount of missing data (e.g. due to clouds) but of course decreases the time accuracy of measurements.\n\nTo extract the samples from the cube, we call extract_geom(), provide the training data and the name of the time column. Please notice that we use quite a few parallel worker instances (even more than the number of cores of the machine) to make downloading the data a little faster. Although the extraction will only need to download smaller patches of images that intersect (in space and time) with the points, it is still the most time consuming part of the extraction when working locally (not on AWS, where the data is stored). As a result, the extraction might take a few minutes on your local machine.\n\ngdalcubes_options(parallel = 12)\ncubex <- extract_geom(cube,training, time_column = \"t\")\nnrow(cubex)\n## [1] 200\n\nOnce available, notice that the number of results is smaller than the number of selected sample points simply because for some points there was no (cloud-free) image available. We can now merge the results with the point features and plot the NDVI by time for the different crop types.\n\nsf = training\nsf$FID = rownames(sf)\ndf = merge(sf, cubex, by = \"FID\")\n\nlibrary(ggplot2)\nlibrary(cowplot)\np = lapply(names, function(x) {\n  df |>\n    dplyr::filter(LC1_NAME == x) |>\n    ggplot( aes(x = as.Date(t), y = NDVI)) +\n    geom_smooth(method = \"loess\") +\n    geom_point(size = 2) + \n    ylim(c(0,1)) + xlim(c(as.Date(\"2018-05-01\"),as.Date(\"2018-09-30\"))) + \n    xlab(\"Time\") + ylab(\"NDVI\")  + ggtitle(x)\n})\nplot_grid(plotlist = p)\n## `geom_smooth()` using formula 'y ~ x'\n## `geom_smooth()` using formula 'y ~ x'\n## `geom_smooth()` using formula 'y ~ x'\n## `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "source/tutorials/vignettes/gc03_ML_training_data.html#further-options-to-extract-data",
    "href": "source/tutorials/vignettes/gc03_ML_training_data.html#further-options-to-extract-data",
    "title": "3. Extracting training data for machine learning models",
    "section": "Further options to extract data",
    "text": "Further options to extract data\nNotice that extraction with extract_geom() is not the only possible way to extract pixels or time series from data cubes. For smaller amounts of samples, extraction can be performed entirely using the available operations crop(), slice_time(), slice_space(), or the general selection operator [. For example, extraction of a few spatiotetemporal points on the LUCAS data (see last example) can also achieved by iterating over all samples and extracting a single point using cube[x$geometry, x$t].\n\nNDVI = NULL\nfor (i in 1:nrow(training)) {\n  y = cube[x$geometry[i],x$t[i]]\n  ndvi_pixel <- as_array(y)\n  NDVI = c(NDVI, as.vector(ndvi_pixel))\n}\n\nThere are of course other applications that require a different preparation of training data. As an example, gdalcubes can also be used for the creation of spatial (or even spatiotemporal) blocks in order to apply convolutional neural networks for object detection, or segmentation tasks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gdalcubes",
    "section": "",
    "text": "It provides functions to create and process four-dimensional regular raster data cubes from irregular image collections, hiding complexities in the data such as different map projections and spatial overlaps of images, or different spatial resolutions of spatial bands from users.\nBuilt-in functions on data cubes include\n\nreduction over space and time,\nfiltering by space, time, and bands,\narithmetic operations on pixels (e.g. to compute vegetation indexes),\nextraction from polygons, points,\nextraction of time-series and computation of zonal statistics,\nconvolutions and moving window aggregation over time,\ngap-filling by time series interpolation,\ncombining variables of identically shaped cubes,\nexecution of external processes on data cube chunks (streaming), and\nexport as netCDF or GeoTIFF file(s),\n\ngdalcubes constructs data cubes on the fly, making it straightforward to go from low resolution experiments to full resolution analyses."
  }
]